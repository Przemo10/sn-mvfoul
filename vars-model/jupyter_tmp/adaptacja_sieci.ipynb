{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ea5973-b1ae-4d52-87cc-88e0bd3d8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from SoccerNet.Evaluation.MV_FoulRecognition import evaluate\n",
    "import torch\n",
    "from dataset import MultiViewDataset\n",
    "from train import trainer, evaluation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from model import MVNetwork\n",
    "from config.classes import EVENT_DICTIONARY, INVERSE_EVENT_DICTIONARY\n",
    "from torchvision.models.video import R3D_18_Weights, MC3_18_Weights\n",
    "from torchvision.models.video import R2Plus1D_18_Weights, S3D_Weights\n",
    "from torchvision.models.video import MViT_V2_S_Weights, MViT_V1_B_Weights\n",
    "from torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights, mvit_v1_b, MViT_V1_B_Weights\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29416dd-6fe5-41a8-9f1c-6e3d54a438d5",
   "metadata": {},
   "source": [
    "```python\n",
    "parser = ArgumentParser(description='my method', formatter_class=ArgumentDefaultsHelpFormatter)    \n",
    "parser.add_argument('--path',   required=True, type=str, help='Path to the dataset folder' )\n",
    "parser.add_argument('--max_epochs',   required=False, type=int,   default=60,     help='Maximum number of epochs' )\n",
    "parser.add_argument('--model_name',   required=False, type=str,   default=\"VARS\",     help='named of the model to save' )\n",
    "parser.add_argument('--batch_size', required=False, type=int,   default=2,     help='Batch size' )\n",
    "parser.add_argument('--LR',       required=False, type=float,   default=1e-04, help='Learning Rate' )\n",
    "parser.add_argument('--GPU',        required=False, type=int,   default=-1,     help='ID of the GPU to use' )\n",
    "parser.add_argument('--max_num_worker',   required=False, type=int,   default=1, help='number of worker to load data')\n",
    "parser.add_argument('--loglevel',   required=False, type=str,   default='INFO', help='logging level')\n",
    "parser.add_argument(\"--continue_training\", required=False, action='store_true', help=\"Continue training\")\n",
    "parser.add_argument(\"--num_views\", required=False, type=int, default=5, help=\"Number of views\")\n",
    "parser.add_argument(\"--data_aug\", required=False, type=str, default=\"Yes\", help=\"Data augmentation\")\n",
    "parser.add_argument(\"--pre_model\", required=False, type=str, default=\"r2plus1d_18\", help=\"Name of the pretrained model\")\n",
    "parser.add_argument(\"--pooling_type\", required=False, type=str, default=\"max\", help=\"Which type of pooling should be done\")\n",
    "parser.add_argument(\"--weighted_loss\", required=False, type=str, default=\"Yes\", help=\"If the loss should be weighted\")\n",
    "parser.add_argument(\"--start_frame\", required=False, type=int, default=0, help=\"The starting frame\")\n",
    "parser.add_argument(\"--end_frame\", required=False, type=int, default=125, help=\"The ending frame\")\n",
    "parser.add_argument(\"--fps\", required=False, type=int, default=25, help=\"Number of frames per second\")\n",
    "parser.add_argument(\"--step_size\", required=False, type=int, default=3, help=\"StepLR parameter\")\n",
    "parser.add_argument(\"--gamma\", required=False, type=float, default=0.1, help=\"StepLR parameter\")\n",
    "parser.add_argument(\"--weight_decay\", required=False, type=float, default=0.001, help=\"Weight decacy\")\n",
    "\n",
    "parser.add_argument(\"--only_evaluation\", required=False, type=int, default=3, help=\"Only evaluation, 0 = on test set, 1 = on chall set, 2 = on both sets and 3 = train/valid/test\")\n",
    "parser.add_argument(\"--path_to_model_weights\", required=False, type=str, default=\"\", help=\"Path to the model weights\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef741967-007e-4855-a72b-928f91d95cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-04\n",
    "gamma = 0.1\n",
    "step_size = 3\n",
    "start_frame = 60\n",
    "end_frame = 87\n",
    "weight_decay = 0.001\n",
    "        \n",
    "model_name = \"VARS\"\n",
    "pre_model = \"mvit_v2_s\"\n",
    "num_views = 2\n",
    "fps = 10\n",
    "number_of_frames = int((end_frame - start_frame) / ((end_frame - start_frame) / (((end_frame - start_frame) / 25) * fps)))\n",
    "number_of_frames2 =  (((end_frame - start_frame) / 25) * fps)\n",
    "batch_size = 4\n",
    "data_aug = 'Yes'\n",
    "path = 'dataset224p'\n",
    "pooling_type = \"max\"\n",
    "weighted_loss = \"Yes\"\n",
    "max_num_worker = 0\n",
    "max_epochs = 50\n",
    "only_evaluation = 3\n",
    "path_to_model_weights = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7e4c52-6a43-4f80-8c53-90b77088cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "    numeric_level = getattr(logging, 'INFO'.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        raise ValueError('Invalid log level: %s' % 'INFO')\n",
    "\n",
    "    os.makedirs(os.path.join(\"models\", os.path.join(model_name, os.path.join(str(num_views), os.path.join(pre_model, os.path.join(str(LR),\n",
    "                            \"_B\" + str(batch_size) + \"_F\" + str(number_of_frames) + \"_S\" + \"_G\" + str(gamma) + \"_Step\" + str(step_size)))))), exist_ok=True)\n",
    "\n",
    "    best_model_path = os.path.join(\"models\", os.path.join(model_name, os.path.join(str(num_views), os.path.join(pre_model, os.path.join(str(LR),\n",
    "                            \"_B\" + str(batch_size) + \"_F\" + str(number_of_frames) + \"_S\" + \"_G\" + str(gamma) + \"_Step\" + str(step_size))))))\n",
    "\n",
    "\n",
    "    log_path = os.path.join(best_model_path, \"logging.log\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=numeric_level,\n",
    "        format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b58c1b2-afd6-4868-a64f-332fd7405a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if data_aug == 'Yes':\n",
    "        transformAug = transforms.Compose([\n",
    "                                          transforms.RandomAffine(degrees=(0, 0), translate=(0.1, 0.1), scale=(0.9, 1)),\n",
    "                                          transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "                                          transforms.RandomRotation(degrees=5),\n",
    "                                          transforms.ColorJitter(brightness=0.5, saturation=0.5, contrast=0.5),\n",
    "                                          transforms.RandomHorizontalFlip()\n",
    "                                          ])\n",
    "    else:\n",
    "        transformAug = None\n",
    "\n",
    "    if pre_model == \"r3d_18\":\n",
    "        transforms_model = R3D_18_Weights.KINETICS400_V1.transforms()        \n",
    "    elif pre_model == \"s3d\":\n",
    "        transforms_model = S3D_Weights.KINETICS400_V1.transforms()       \n",
    "    elif pre_model == \"mc3_18\":\n",
    "        transforms_model = MC3_18_Weights.KINETICS400_V1.transforms()       \n",
    "    elif pre_model == \"r2plus1d_18\":\n",
    "        transforms_model = R2Plus1D_18_Weights.KINETICS400_V1.transforms()\n",
    "    elif pre_model == \"mvit_v2_s\":\n",
    "        transforms_model = MViT_V2_S_Weights.KINETICS400_V1.transforms()\n",
    "    else:\n",
    "        transforms_model = R2Plus1D_18_Weights.KINETICS400_V1.transforms()\n",
    "        print(\"Warning: Could not find the desired pretrained model\")\n",
    "        print(\"Possible options are: r3d_18, s3d, mc3_18, mvit_v2_s and r2plus1d_18\")\n",
    "        print(\"We continue with r2plus1d_18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f1b6bd-37e8-406a-b50a-de33548b5dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoClassification(\n",
       "    crop_size=[224, 224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.45, 0.45, 0.45]\n",
       "    std=[0.225, 0.225, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4581a3c2-4efd-4400-8031-a56b9898cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319\n",
      "321\n"
     ]
    }
   ],
   "source": [
    "        dataset_Train = MultiViewDataset(path=path, start=start_frame, end=end_frame, fps=fps, split='train',\n",
    "            num_views = num_views, transform=transformAug, transform_model=transforms_model)\n",
    "        dataset_Valid2 = MultiViewDataset(path=path, start=start_frame, end=end_frame, fps=fps, split='valid', num_views = 5,\n",
    "            transform_model=transforms_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b60d9a1-b925-47dc-b818-e69a17c8d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 11, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(dataset_Train.__getitem__(49)[2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "177fdc7e-56df-4100-b521-f88571a4c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Create the dataloaders for train validation and test datasets\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_Train,\n",
    "            batch_size=batch_size, shuffle=False,\n",
    "            num_workers=max_num_worker, pin_memory=True)\n",
    "\n",
    "        val_loader2 = torch.utils.data.DataLoader(dataset_Valid2,\n",
    "            batch_size=1, shuffle=True,\n",
    "            num_workers=max_num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480d49f6-5a2f-448c-b400-0bf470b95090",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8290d31b-7d68-4d72-934f-88dc35b5a947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter[0], train_iter[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7181f3ad-4332-44ab-afc5-e5b6d7f3ca15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " torch.Size([4, 8]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter[1], train_iter[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b53cfd7a-3ef7-4919-b16a-d93987212c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 11, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter[2].shape # batch size, views, channels, time/depth, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ee8997d-75e4-4353-8e17-0b0ffb8ab195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter[3] # lista zdjec na dany batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e1cac1-67e2-4458-8be7-d039160b231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = next(iter(val_loader2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66072b9-319b-46d1-945f-1de55bdfbc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 11, 224, 224]), ['71'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_iter[2].shape, val_iter[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e7ea3db-a71a-4a9b-a5bc-4b3752017a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MVNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b9505a-056e-45ba-b68d-f2fe87ed7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = \"mvit_v2_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50d0d48-a76e-4196-8223-18f5e4e8f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MVNetwork(net_name=pre_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4390dd84-402b-445a-b10c-4dc78665f16b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m videos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vars/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SocerNetProject/sn-mvfoul/vars-model/model.py:51\u001b[0m, in \u001b[0;36mMVNetwork.forward\u001b[0;34m(self, mvimages)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mvimages):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmvnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vars/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SocerNetProject/sn-mvfoul/vars-model/mvaggregate.py:122\u001b[0m, in \u001b[0;36mMVAggregate.forward\u001b[0;34m(self, mvimages)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mvimages):\n\u001b[0;32m--> 122\u001b[0m     pooled_view, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmvimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     inter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter(pooled_view)\n\u001b[1;32m    125\u001b[0m     pred_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_action(inter)\n",
      "File \u001b[0;32m~/miniconda3/envs/vars/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SocerNetProject/sn-mvfoul/vars-model/mvaggregate.py:70\u001b[0m, in \u001b[0;36mViewMaxAggregate.forward\u001b[0;34m(self, mvimages)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mvimages):\n\u001b[0;32m---> 70\u001b[0m     B, V, C, D, H, W \u001b[38;5;241m=\u001b[39m mvimages\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# Batch, Views, Channel, Depth, Height, Width\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlifting_net(unbatch_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_tensor(mvimages, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, squeeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)), B, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, unsqueeze\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     72\u001b[0m     pooled_view \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(aux, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 5)"
     ]
    }
   ],
   "source": [
    "videos = torch.randn(4,2, 3, 11, 224, 224)\n",
    "output = model(videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14d383-e434-46ee-8585-cd7bae3f047d",
   "metadata": {},
   "source": [
    "## Metoda ze starego algorytmu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "183355e5-c652-464c-813c-2225c9825fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video import r2plus1d_18, R2Plus1D_18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ee2cd8d-5056-4707-8e9a-fd211ec17d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model = R2Plus1D_18_Weights.DEFAULT\n",
    "network = r2plus1d_18(weights=weights_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa2831f3-b5ae-4065-988b-81e048318049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import batch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5eb34fa1-3d71-4e40-a6f7-ecbf75a3b266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 11, 224, 224])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tensor(train_iter[2],dim=1,squeeze=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f671f-715b-40da-bc51-daef79ca27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "network(batch_tensor(train_iter[2],dim=1,squeeze=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894fc16-76b0-4994-9d35-a726a830705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network(val_iter[2]).shape  # wywali blad bo tensor 5 D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec574a-ca8f-400f-ac26-32c6cee4b56d",
   "metadata": {},
   "source": [
    "## Nowy algorytm - siec debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ffdc055-d98f-4676-be0d-306a91d3af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_model = MViT_V2_S_Weights.DEFAULT\n",
    "model = mvit_v2_s(weights=weights_model)\n",
    "n=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dba93aa-5f22-4964-b377-c360de793b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07238e88-e1b4-44c9-9150-d9322dc0d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_proj.weight torch.Size([96, 3, 3, 7, 7])\n",
      "conv_proj.bias torch.Size([96])\n",
      "pos_encoding.class_token torch.Size([96])\n",
      "blocks.0.norm1.weight torch.Size([96])\n",
      "blocks.0.norm1.bias torch.Size([96])\n",
      "blocks.0.norm2.weight torch.Size([96])\n",
      "blocks.0.norm2.bias torch.Size([96])\n",
      "blocks.0.attn.rel_pos_h torch.Size([111, 96])\n",
      "blocks.0.attn.rel_pos_w torch.Size([111, 96])\n",
      "blocks.0.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.0.attn.qkv.weight torch.Size([288, 96])\n",
      "blocks.0.attn.qkv.bias torch.Size([288])\n",
      "blocks.0.attn.project.0.weight torch.Size([96, 96])\n",
      "blocks.0.attn.project.0.bias torch.Size([96])\n",
      "blocks.0.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.0.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.0.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.0.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.0.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.0.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.0.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.0.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.0.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.0.mlp.0.weight torch.Size([384, 96])\n",
      "blocks.0.mlp.0.bias torch.Size([384])\n",
      "blocks.0.mlp.3.weight torch.Size([96, 384])\n",
      "blocks.0.mlp.3.bias torch.Size([96])\n",
      "blocks.1.norm1.weight torch.Size([96])\n",
      "blocks.1.norm1.bias torch.Size([96])\n",
      "blocks.1.norm2.weight torch.Size([192])\n",
      "blocks.1.norm2.bias torch.Size([192])\n",
      "blocks.1.attn.rel_pos_h torch.Size([55, 96])\n",
      "blocks.1.attn.rel_pos_w torch.Size([55, 96])\n",
      "blocks.1.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.1.attn.qkv.weight torch.Size([576, 96])\n",
      "blocks.1.attn.qkv.bias torch.Size([576])\n",
      "blocks.1.attn.project.0.weight torch.Size([192, 192])\n",
      "blocks.1.attn.project.0.bias torch.Size([192])\n",
      "blocks.1.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.1.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.1.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.1.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.1.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.1.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.1.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.1.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.1.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.1.mlp.0.weight torch.Size([768, 192])\n",
      "blocks.1.mlp.0.bias torch.Size([768])\n",
      "blocks.1.mlp.3.weight torch.Size([192, 768])\n",
      "blocks.1.mlp.3.bias torch.Size([192])\n",
      "blocks.1.project.weight torch.Size([192, 96])\n",
      "blocks.1.project.bias torch.Size([192])\n",
      "blocks.2.norm1.weight torch.Size([192])\n",
      "blocks.2.norm1.bias torch.Size([192])\n",
      "blocks.2.norm2.weight torch.Size([192])\n",
      "blocks.2.norm2.bias torch.Size([192])\n",
      "blocks.2.attn.rel_pos_h torch.Size([55, 96])\n",
      "blocks.2.attn.rel_pos_w torch.Size([55, 96])\n",
      "blocks.2.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.2.attn.qkv.weight torch.Size([576, 192])\n",
      "blocks.2.attn.qkv.bias torch.Size([576])\n",
      "blocks.2.attn.project.0.weight torch.Size([192, 192])\n",
      "blocks.2.attn.project.0.bias torch.Size([192])\n",
      "blocks.2.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.2.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.2.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.2.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.2.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.2.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.2.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.2.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.2.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.2.mlp.0.weight torch.Size([768, 192])\n",
      "blocks.2.mlp.0.bias torch.Size([768])\n",
      "blocks.2.mlp.3.weight torch.Size([192, 768])\n",
      "blocks.2.mlp.3.bias torch.Size([192])\n",
      "blocks.3.norm1.weight torch.Size([192])\n",
      "blocks.3.norm1.bias torch.Size([192])\n",
      "blocks.3.norm2.weight torch.Size([384])\n",
      "blocks.3.norm2.bias torch.Size([384])\n",
      "blocks.3.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.3.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.3.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.3.attn.qkv.weight torch.Size([1152, 192])\n",
      "blocks.3.attn.qkv.bias torch.Size([1152])\n",
      "blocks.3.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.3.attn.project.0.bias torch.Size([384])\n",
      "blocks.3.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.3.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.3.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.3.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.3.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.3.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.3.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.3.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.3.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.3.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.3.mlp.0.bias torch.Size([1536])\n",
      "blocks.3.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.3.mlp.3.bias torch.Size([384])\n",
      "blocks.3.project.weight torch.Size([384, 192])\n",
      "blocks.3.project.bias torch.Size([384])\n",
      "blocks.4.norm1.weight torch.Size([384])\n",
      "blocks.4.norm1.bias torch.Size([384])\n",
      "blocks.4.norm2.weight torch.Size([384])\n",
      "blocks.4.norm2.bias torch.Size([384])\n",
      "blocks.4.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.4.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.4.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.4.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.4.attn.qkv.bias torch.Size([1152])\n",
      "blocks.4.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.4.attn.project.0.bias torch.Size([384])\n",
      "blocks.4.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.4.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.4.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.4.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.4.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.4.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.4.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.4.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.4.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.4.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.4.mlp.0.bias torch.Size([1536])\n",
      "blocks.4.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.4.mlp.3.bias torch.Size([384])\n",
      "blocks.5.norm1.weight torch.Size([384])\n",
      "blocks.5.norm1.bias torch.Size([384])\n",
      "blocks.5.norm2.weight torch.Size([384])\n",
      "blocks.5.norm2.bias torch.Size([384])\n",
      "blocks.5.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.5.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.5.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.5.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.5.attn.qkv.bias torch.Size([1152])\n",
      "blocks.5.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.5.attn.project.0.bias torch.Size([384])\n",
      "blocks.5.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.5.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.5.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.5.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.5.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.5.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.5.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.5.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.5.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.5.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.5.mlp.0.bias torch.Size([1536])\n",
      "blocks.5.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.5.mlp.3.bias torch.Size([384])\n",
      "blocks.6.norm1.weight torch.Size([384])\n",
      "blocks.6.norm1.bias torch.Size([384])\n",
      "blocks.6.norm2.weight torch.Size([384])\n",
      "blocks.6.norm2.bias torch.Size([384])\n",
      "blocks.6.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.6.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.6.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.6.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.6.attn.qkv.bias torch.Size([1152])\n",
      "blocks.6.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.6.attn.project.0.bias torch.Size([384])\n",
      "blocks.6.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.6.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.6.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.6.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.6.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.6.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.6.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.6.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.6.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.6.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.6.mlp.0.bias torch.Size([1536])\n",
      "blocks.6.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.6.mlp.3.bias torch.Size([384])\n",
      "blocks.7.norm1.weight torch.Size([384])\n",
      "blocks.7.norm1.bias torch.Size([384])\n",
      "blocks.7.norm2.weight torch.Size([384])\n",
      "blocks.7.norm2.bias torch.Size([384])\n",
      "blocks.7.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.7.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.7.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.7.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.7.attn.qkv.bias torch.Size([1152])\n",
      "blocks.7.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.7.attn.project.0.bias torch.Size([384])\n",
      "blocks.7.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.7.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.7.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.7.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.7.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.7.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.7.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.7.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.7.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.7.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.7.mlp.0.bias torch.Size([1536])\n",
      "blocks.7.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.7.mlp.3.bias torch.Size([384])\n",
      "blocks.8.norm1.weight torch.Size([384])\n",
      "blocks.8.norm1.bias torch.Size([384])\n",
      "blocks.8.norm2.weight torch.Size([384])\n",
      "blocks.8.norm2.bias torch.Size([384])\n",
      "blocks.8.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.8.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.8.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.8.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.8.attn.qkv.bias torch.Size([1152])\n",
      "blocks.8.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.8.attn.project.0.bias torch.Size([384])\n",
      "blocks.8.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.8.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.8.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.8.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.8.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.8.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.8.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.8.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.8.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.8.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.8.mlp.0.bias torch.Size([1536])\n",
      "blocks.8.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.8.mlp.3.bias torch.Size([384])\n",
      "blocks.9.norm1.weight torch.Size([384])\n",
      "blocks.9.norm1.bias torch.Size([384])\n",
      "blocks.9.norm2.weight torch.Size([384])\n",
      "blocks.9.norm2.bias torch.Size([384])\n",
      "blocks.9.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.9.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.9.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.9.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.9.attn.qkv.bias torch.Size([1152])\n",
      "blocks.9.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.9.attn.project.0.bias torch.Size([384])\n",
      "blocks.9.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.9.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.9.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.9.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.9.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.9.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.9.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.9.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.9.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.9.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.9.mlp.0.bias torch.Size([1536])\n",
      "blocks.9.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.9.mlp.3.bias torch.Size([384])\n",
      "blocks.10.norm1.weight torch.Size([384])\n",
      "blocks.10.norm1.bias torch.Size([384])\n",
      "blocks.10.norm2.weight torch.Size([384])\n",
      "blocks.10.norm2.bias torch.Size([384])\n",
      "blocks.10.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.10.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.10.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.10.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.10.attn.qkv.bias torch.Size([1152])\n",
      "blocks.10.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.10.attn.project.0.bias torch.Size([384])\n",
      "blocks.10.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.10.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.10.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.10.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.10.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.10.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.10.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.10.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.10.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.10.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.10.mlp.0.bias torch.Size([1536])\n",
      "blocks.10.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.10.mlp.3.bias torch.Size([384])\n",
      "blocks.11.norm1.weight torch.Size([384])\n",
      "blocks.11.norm1.bias torch.Size([384])\n",
      "blocks.11.norm2.weight torch.Size([384])\n",
      "blocks.11.norm2.bias torch.Size([384])\n",
      "blocks.11.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.11.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.11.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.11.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.11.attn.qkv.bias torch.Size([1152])\n",
      "blocks.11.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.11.attn.project.0.bias torch.Size([384])\n",
      "blocks.11.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.11.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.11.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.11.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.11.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.11.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.11.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.11.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.11.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.11.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.11.mlp.0.bias torch.Size([1536])\n",
      "blocks.11.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.11.mlp.3.bias torch.Size([384])\n",
      "blocks.12.norm1.weight torch.Size([384])\n",
      "blocks.12.norm1.bias torch.Size([384])\n",
      "blocks.12.norm2.weight torch.Size([384])\n",
      "blocks.12.norm2.bias torch.Size([384])\n",
      "blocks.12.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.12.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.12.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.12.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.12.attn.qkv.bias torch.Size([1152])\n",
      "blocks.12.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.12.attn.project.0.bias torch.Size([384])\n",
      "blocks.12.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.12.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.12.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.12.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.12.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.12.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.12.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.12.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.12.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.12.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.12.mlp.0.bias torch.Size([1536])\n",
      "blocks.12.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.12.mlp.3.bias torch.Size([384])\n",
      "blocks.13.norm1.weight torch.Size([384])\n",
      "blocks.13.norm1.bias torch.Size([384])\n",
      "blocks.13.norm2.weight torch.Size([384])\n",
      "blocks.13.norm2.bias torch.Size([384])\n",
      "blocks.13.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.13.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.13.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.13.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.13.attn.qkv.bias torch.Size([1152])\n",
      "blocks.13.attn.project.0.weight torch.Size([384, 384])\n",
      "blocks.13.attn.project.0.bias torch.Size([384])\n",
      "blocks.13.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.13.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.13.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.13.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.13.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.13.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.13.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.13.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.13.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.13.mlp.0.weight torch.Size([1536, 384])\n",
      "blocks.13.mlp.0.bias torch.Size([1536])\n",
      "blocks.13.mlp.3.weight torch.Size([384, 1536])\n",
      "blocks.13.mlp.3.bias torch.Size([384])\n",
      "blocks.14.norm1.weight torch.Size([384])\n",
      "blocks.14.norm1.bias torch.Size([384])\n",
      "blocks.14.norm2.weight torch.Size([768])\n",
      "blocks.14.norm2.bias torch.Size([768])\n",
      "blocks.14.attn.rel_pos_h torch.Size([27, 96])\n",
      "blocks.14.attn.rel_pos_w torch.Size([27, 96])\n",
      "blocks.14.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.14.attn.qkv.weight torch.Size([2304, 384])\n",
      "blocks.14.attn.qkv.bias torch.Size([2304])\n",
      "blocks.14.attn.project.0.weight torch.Size([768, 768])\n",
      "blocks.14.attn.project.0.bias torch.Size([768])\n",
      "blocks.14.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.14.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.14.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.14.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.14.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.14.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.14.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.14.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.14.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.14.mlp.0.weight torch.Size([3072, 768])\n",
      "blocks.14.mlp.0.bias torch.Size([3072])\n",
      "blocks.14.mlp.3.weight torch.Size([768, 3072])\n",
      "blocks.14.mlp.3.bias torch.Size([768])\n",
      "blocks.14.project.weight torch.Size([768, 384])\n",
      "blocks.14.project.bias torch.Size([768])\n",
      "blocks.15.norm1.weight torch.Size([768])\n",
      "blocks.15.norm1.bias torch.Size([768])\n",
      "blocks.15.norm2.weight torch.Size([768])\n",
      "blocks.15.norm2.bias torch.Size([768])\n",
      "blocks.15.attn.rel_pos_h torch.Size([13, 96])\n",
      "blocks.15.attn.rel_pos_w torch.Size([13, 96])\n",
      "blocks.15.attn.rel_pos_t torch.Size([15, 96])\n",
      "blocks.15.attn.qkv.weight torch.Size([2304, 768])\n",
      "blocks.15.attn.qkv.bias torch.Size([2304])\n",
      "blocks.15.attn.project.0.weight torch.Size([768, 768])\n",
      "blocks.15.attn.project.0.bias torch.Size([768])\n",
      "blocks.15.attn.pool_q.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.15.attn.pool_q.norm_act.0.weight torch.Size([96])\n",
      "blocks.15.attn.pool_q.norm_act.0.bias torch.Size([96])\n",
      "blocks.15.attn.pool_k.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.15.attn.pool_k.norm_act.0.weight torch.Size([96])\n",
      "blocks.15.attn.pool_k.norm_act.0.bias torch.Size([96])\n",
      "blocks.15.attn.pool_v.pool.weight torch.Size([96, 1, 3, 3, 3])\n",
      "blocks.15.attn.pool_v.norm_act.0.weight torch.Size([96])\n",
      "blocks.15.attn.pool_v.norm_act.0.bias torch.Size([96])\n",
      "blocks.15.mlp.0.weight torch.Size([3072, 768])\n",
      "blocks.15.mlp.0.bias torch.Size([3072])\n",
      "blocks.15.mlp.3.weight torch.Size([768, 3072])\n",
      "blocks.15.mlp.3.bias torch.Size([768])\n",
      "norm.weight torch.Size([768])\n",
      "norm.bias torch.Size([768])\n",
      "head.1.weight torch.Size([400, 768])\n",
      "head.1.bias torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c026bdc-243f-4f6f-97d9-0efe6a183a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head[1].in_features # tu nie wiem czy 96, czy 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b68f4fb-5905-4d70-b2c0-3fc35ba4b16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3, 11, 224, 224])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x = train_iter[2]\n",
    "input_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21c53bc6-4ffa-4d99-96a8-46b767bdd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = einops.rearrange(input_x, 'b n c d h w -> (b n) c d h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10a0bc41-19e1-41da-8847-cb1c53c461c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 11, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fdc4ef2-a691-43a0-964a-01222c49c7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 96, 6, 56, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 56, 56])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv_proj(input_x)\n",
    "print(x.shape)\n",
    "thw_shape = x.shape[2:]\n",
    "thw_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8a4cda6-e2fb-4190-b2c3-d49d2d373f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, D, H, W = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b200429-05a7-4341-9f79-ba481dfbfda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18816, 96])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(B, C, D * H * W).transpose(1,2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12de46df-5c60-42ba-8f1e-c4f18a789fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos=model.pos_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74f63fd6-b8cf-49fa-a855-439c3fe36a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18817, 96])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eef6707-0209-4b28-b70b-92557427de85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18817"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_frame = x_pos.shape[1];tokens_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d5fd6af-a91a-4b6d-b3b5-40dc6fec9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {'single': {}}\n",
    "if n > 1:\n",
    "    output_dict['mv_collection'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4b9080f-d460-46c0-8036-4d5c96b5a4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'single': {}, 'mv_collection': {}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18e638d5-9c9a-43e4-8e93-e5340ac0ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 18817, 96]) (6, 56, 56)\n",
      "torch.Size([8, 4705, 192]) (6, 28, 28)\n",
      "torch.Size([8, 4705, 192]) (6, 28, 28)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 1177, 384]) (6, 14, 14)\n",
      "torch.Size([8, 295, 768]) (6, 7, 7)\n",
      "torch.Size([8, 295, 768]) (6, 7, 7)\n",
      "torch.Size([8, 295, 768])\n"
     ]
    }
   ],
   "source": [
    "tokens = x_pos.clone()\n",
    "for block in model.blocks:\n",
    "    tokens, thw_shape = block(tokens, thw_shape)\n",
    "    print(tokens.shape, thw_shape)\n",
    "\n",
    "tokens = model.norm(tokens)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0c787f5-8407-4b74-a08b-dd2ab0c58efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 295, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f25b1eca-fca1-4474-82cb-cc46180cc399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18818, 96])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pos_encoding(x_pos).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7289ff82-bc81-4f6f-846a-b6a74a9468db",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def format_multi_frame_tokens( x, batch_size, tokens_per_frame):\n",
    "        \"\"\"\n",
    "        Formats the tokens for multiple frames.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape [batch_size * n, tokens, channels].\n",
    "            batch_size (int): Original batch size.\n",
    "            tokens_per_frame (int): Number of tokens per frame.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Formatted tensor.\n",
    "        \"\"\"\n",
    "        # Rearrange the tensor to merge batch and frame dimensions\n",
    "        print(f\"Initial shape of x: {x.shape}\")\n",
    "        x = einops.rearrange(x, '(b n) s c -> b (n s) c', b=batch_size, n=n)\n",
    "        first_img_token_idx = 0\n",
    "\n",
    "        print(f\"Shape after rearrange: {x.shape}\")\n",
    "\n",
    "        # Handle cls_token if present\n",
    "        if hasattr(model.pos_encoding, 'class_token'):\n",
    "            for i in range(1, n):\n",
    "                excess_cls_index = i * tokens_per_frame + 1\n",
    "                x = torch.cat((x[:, :excess_cls_index], x[:, excess_cls_index + 2:]), dim=1)\n",
    "                print(f\"Shape after removing cls token at frame {i}: {x.shape}\")\n",
    "            first_img_token_idx = 1\n",
    "\n",
    "        # Normalize and add image embeddings\n",
    "        image_embeddings = F.normalize(img_embed_matrix, dim=-1)\n",
    "        print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "        # Repeat embeddings to match the number of tokens per frame\n",
    "        repeated_embeddings = torch.repeat_interleave(image_embeddings, tokens_per_frame-1 , dim=1)\n",
    "        print(f\"Repeated embeddings shape: {repeated_embeddings.shape}\")\n",
    "\n",
    "        x[:, first_img_token_idx:] += repeated_embeddings\n",
    "        print(f\"Shape after adding image embeddings: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7575ee6e-2022-4ccc-9aee-19b9e275870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(model.pos_encoding, 'class_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb2227a9-7ffd-4ddc-8540-7d252792b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af32d133-8ad9-43d5-bb69-37ee861ea182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-8.7425e-02, -7.0894e-02,  6.7221e-02,  1.3171e-01,  1.1597e-01,\n",
       "           7.3686e-02,  3.2308e-02,  8.2645e-02, -8.2328e-02,  7.1367e-02,\n",
       "          -3.5731e-02,  6.5479e-02, -1.1185e-01,  2.1948e-02,  5.6373e-02,\n",
       "          -1.1769e-02, -2.1720e-02, -1.1829e-01, -4.9559e-02, -2.9565e-03,\n",
       "          -9.4372e-02,  2.0462e-02, -5.5385e-02, -5.1856e-02, -4.8786e-02,\n",
       "          -1.2847e-01,  1.2518e-01, -3.7235e-02,  3.6176e-02,  4.1613e-02,\n",
       "          -8.1544e-02, -4.9512e-03,  7.6256e-02,  1.3083e-01,  1.2910e-01,\n",
       "          -2.4532e-02,  1.2459e-01, -2.8243e-02, -7.8253e-03,  9.3521e-02,\n",
       "           1.6084e-02,  1.0721e-01,  1.3785e-01, -4.4411e-02, -9.4780e-02,\n",
       "           1.2711e-02,  9.7135e-02, -9.8067e-02,  1.0470e-01,  2.4907e-02,\n",
       "          -2.5587e-02,  1.3250e-01, -4.6502e-02,  4.9412e-02, -1.2213e-01,\n",
       "          -4.6024e-02, -1.2305e-02, -5.9592e-02, -6.8150e-02,  3.2270e-02,\n",
       "          -8.0385e-02, -1.4049e-01,  6.2135e-02, -1.7336e-02, -8.2488e-02,\n",
       "           5.8340e-02,  9.8760e-03,  9.3372e-02,  1.2259e-01,  9.3262e-02,\n",
       "           9.9703e-03,  1.2326e-01, -9.5789e-02,  9.2027e-03,  8.1693e-02,\n",
       "          -1.1758e-02, -1.3129e-01, -4.7211e-02, -2.5324e-03,  1.3609e-01,\n",
       "          -1.0774e-01,  8.0108e-02,  1.2001e-01, -1.3981e-01, -7.5678e-02,\n",
       "          -1.0757e-01, -9.9574e-03,  1.5180e-02, -2.5445e-03,  1.9697e-02,\n",
       "          -5.4363e-02, -8.8011e-02,  6.5758e-02,  9.9363e-02, -6.6771e-02,\n",
       "           5.3041e-02],\n",
       "         [ 5.9782e-02,  1.4406e-01, -3.1037e-02, -8.6777e-02, -7.8186e-02,\n",
       "          -1.0108e-01,  1.3722e-01, -7.9895e-02,  1.8667e-02, -1.0975e-01,\n",
       "          -6.9400e-02, -1.0731e-01,  8.4694e-02,  5.7312e-02,  4.1118e-02,\n",
       "           7.6397e-02, -1.3029e-01,  4.3551e-02,  1.0812e-01,  8.1636e-02,\n",
       "          -1.3186e-01, -3.7155e-02, -9.7927e-03,  1.0218e-01, -4.7284e-03,\n",
       "           1.3600e-01, -5.5611e-02, -3.7956e-02,  1.2041e-01, -6.8301e-02,\n",
       "          -5.3970e-02,  1.2578e-01,  9.7000e-02,  3.5522e-02,  6.0985e-03,\n",
       "          -8.2143e-02,  1.1393e-01, -2.5092e-04,  1.0180e-01, -1.2735e-01,\n",
       "          -5.1381e-02, -1.2166e-01,  3.7450e-02, -4.0676e-02, -5.3958e-02,\n",
       "           9.6339e-02, -1.4113e-01, -7.7403e-04,  7.7235e-03, -4.6243e-02,\n",
       "          -1.0399e-01, -8.9330e-02, -9.3507e-02, -3.9233e-02,  1.3533e-01,\n",
       "           5.3700e-02, -1.7240e-02, -6.3943e-02,  6.3458e-02, -4.1667e-02,\n",
       "          -6.0700e-02,  1.3775e-01, -1.1115e-01,  1.1385e-01, -1.4678e-02,\n",
       "          -6.4763e-02,  1.3966e-01, -8.1166e-02,  1.1560e-01,  7.9134e-02,\n",
       "          -8.4535e-02, -9.5555e-02,  1.4199e-01, -4.9996e-02,  4.3421e-02,\n",
       "           2.6894e-02, -3.4465e-02, -1.0830e-01,  1.1168e-01,  1.4065e-01,\n",
       "           1.3289e-01,  8.3339e-02, -1.0781e-01, -7.7565e-03, -4.9097e-02,\n",
       "           4.3983e-02,  3.6412e-02,  1.3216e-01, -8.9550e-03, -4.7357e-02,\n",
       "           4.4152e-05,  2.2897e-02, -2.7239e-02,  8.3135e-02,  5.5894e-02,\n",
       "           5.1812e-02]]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_embed_matrix = nn.Parameter(torch.zeros(1, n, embed_dim), requires_grad=True)\n",
    "nn.init.xavier_uniform_(img_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621589ca-5fe5-4e34-949f-ddbcbf6209c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embed_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f87c3-cf20-403a-8b04-d5a46a72478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699f1fb-49a3-44d9-8648-138e4b0b12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = format_multi_frame_tokens( x_pos, batch_size, tokens_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee78838-75d3-42d1-85c2-ad87c4f10339",
   "metadata": {},
   "outputs": [],
   "source": [
    "thw_shape = x.shape[2:]\n",
    "for block in model.blocks:\n",
    "    tokens, thw_shape = block(t1, thw_shape)\n",
    "    print(tokens.shape, thw_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c8181-f2c1-425f-a12c-65fd4e92bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "from torchvision.models.video import mvit_v2_s, MViT_V2_S_Weights\n",
    "\n",
    "class MultiVideoHybridMVit2(nn.Module):\n",
    "    \"\"\"\n",
    "    A hybrid model for handling multiple frames per video using MVit_v2_s.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes.\n",
    "        n (int): Number of views per sample.\n",
    "        pretrained_weights (str): Path to the pretrained weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, n, pretrained_weights=None):\n",
    "        super(MultiVideoHybridMVit2, self).__init__()\n",
    "\n",
    "        self.n = n\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize the base MVit_v2_s model\n",
    "        weights = MViT_V2_S_Weights.DEFAULT if pretrained_weights is None else pretrained_weights\n",
    "        self.model = mvit_v2_s(weights=weights)\n",
    "        self.embed_dim = 96\n",
    "\n",
    "        # Initialize the learnable image embedding matrix\n",
    "        self.img_embed_matrix = nn.Parameter(torch.zeros(1, n, self.embed_dim), requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.img_embed_matrix)\n",
    "\n",
    "        # Initialize the classification head\n",
    "        self.classifier = nn.Linear(self.model.head[1].in_features, self.num_classes)\n",
    "        nn.init.zeros_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def format_multi_frame_tokens(self, x, batch_size, tokens_per_frame):\n",
    "        \"\"\"\n",
    "        Formats the tokens for multiple frames.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape [batch_size * n, tokens, embed_dim].\n",
    "            batch_size (int): Original batch size.\n",
    "            tokens_per_frame (int): Number of tokens per frame.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Formatted tensor.\n",
    "        \"\"\"\n",
    "        # Initial shape of x: [batch_size * n, tokens, embed_dim]\n",
    "        # Example initial shape: [8, 18817, 96]\n",
    "        print(f\"Initial shape of x: {x.shape}\")\n",
    "        \n",
    "        # Rearrange the tensor to merge batch and frame dimensions\n",
    "        x = einops.rearrange(x, '(b n) s c -> b (n s) c', b=batch_size, n=self.n)\n",
    "        # Shape after rearrange: [4, 37634, 96] if n=2 (concatenating tokens for all frames per batch)\n",
    "        print(f\"Shape after rearrange: {x.shape}\")\n",
    "\n",
    "        first_img_token_idx = 0\n",
    "\n",
    "        # Handle cls_token if present\n",
    "        if hasattr(self.model.pos_encoding, 'class_token'):\n",
    "            for i in range(1, self.n):\n",
    "                excess_cls_index = i * tokens_per_frame + 1\n",
    "                x = torch.cat((x[:, :excess_cls_index], x[:, excess_cls_index + 2:]), dim=1)\n",
    "                print(f\"Shape after removing cls token at frame {i}: {x.shape}\")\n",
    "            first_img_token_idx = 2\n",
    "            # Shape after removing excess cls tokens: [4, 37633, 96] if n=2 and cls tokens are removed\n",
    "\n",
    "        # Normalize and add image embeddings\n",
    "        image_embeddings = F.normalize(self.img_embed_matrix, dim=-1)\n",
    "        # image_embeddings shape: [1, 2, 96] if n=2\n",
    "        print(f\"Image embeddings shape: {image_embeddings.shape}\")\n",
    "        \n",
    "        # Repeat embeddings to match the number of tokens per frame\n",
    "        repeated_embeddings = torch.repeat_interleave(image_embeddings, tokens_per_frame-2, dim=1)\n",
    "        print(f\"Repeated embeddings shape: {repeated_embeddings.shape}\")\n",
    "\n",
    "        x[:, first_img_token_idx:] += repeated_embeddings\n",
    "        # Shape after adding image embeddings: [4, 37633, 96] (no change in shape, just adding embeddings)\n",
    "        print(f\"Shape after adding image embeddings: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with shape [batch_size, n, channels, depth, height, width].\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with logits for single and multi-view collections.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        output_dict = {'single': {}}\n",
    "        if self.n > 1:\n",
    "            output_dict['mv_collection'] = {}\n",
    "\n",
    "        # Input shape: [batch_size, num_views, channels, depth, height, width]\n",
    "        # Example input shape: [4, 2, 3, 11, 224, 224]\n",
    "        \n",
    "        # Flatten the views into individual images\n",
    "        x = einops.rearrange(x, 'b n c d h w -> (b n) c d h w')\n",
    "        # Shape after rearrange: [8, 3, 11, 224, 224]\n",
    "        print(f\"Shape after rearrange (views to individual images): {x.shape}\")\n",
    "\n",
    "        # Pass through the initial convolutional layers of MVIT to get patch embeddings\n",
    "        x = self.model.conv_proj(x)\n",
    "        # Shape after conv_proj: [8, 96, 6, 56, 56]\n",
    "        print(f\"Shape after conv_proj: {x.shape}\")\n",
    "        \n",
    "        # Get the shape for temporal, height, and width dimensions\n",
    "        init_thw_shape = x.shape[2:]  # Shape: [6, 56, 56]\n",
    "        thw_shape = init_thw_shape\n",
    "        print(f\"THW shape after conv_proj: {thw_shape}\")\n",
    "\n",
    "        # Flatten the spatial dimensions and bring channels to the last dimension\n",
    "        B, C, D, H, W = x.shape\n",
    "        x = x.view(B, C, D * H * W).transpose(1, 2)  # Now x has shape [batch_size, num_tokens, embed_dim]\n",
    "        # Shape after view and transpose: [8, 18816, 96]\n",
    "        print(f\"Shape after view and transpose: {x.shape}\")\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.model.pos_encoding(x)\n",
    "        print(f\"Shape after adding positional encoding: {x.shape}\")\n",
    "\n",
    "        tokens_per_frame = x.shape[1]  # Number of tokens per frame\n",
    "        \n",
    "        for view_type in output_dict:\n",
    "            tokens = x.clone()\n",
    "            if view_type == 'mv_collection':\n",
    "                tokens = self.format_multi_frame_tokens(tokens, batch_size, tokens_per_frame)\n",
    "                # Shape after format_multi_frame_tokens: [4, 37632, 96] if n=2\n",
    "                print(f\"Shape after format_multi_frame_tokens: {tokens.shape}\")\n",
    "                # Update thw_shape after merging frames\n",
    "                thw_shape = init_thw_shape\n",
    "                print(f\"Updated thw_shape for mv_collection: {thw_shape}\")\n",
    "\n",
    "            # Sequentially pass the tokens through each block with the thw argument\n",
    "            for block in self.model.blocks:\n",
    "                tokens, thw_shape = block(tokens, thw_shape)\n",
    "                print(tokens.shape, thw_shape)  # Debug print statement for shape tracking\n",
    "                # Shape after each block will be [batch_size, num_tokens, embed_dim]\n",
    "\n",
    "            tokens = self.model.norm(tokens)\n",
    "            # Shape after normalization: [4, 295, 768] if final number of tokens is 295 and embed_dim is 768\n",
    "            print(f\"Shape after normalization: {tokens.shape}\")\n",
    "\n",
    "            logits = self.classifier(tokens[:, 0])\n",
    "            # Shape of logits: [batch_size, num_classes], e.g., [4, 10]\n",
    "            print(f\"Shape of logits: {logits.shape}\")\n",
    "\n",
    "            output_dict[view_type]['logits'] = logits\n",
    "\n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba05e7-bca2-4929-8d8d-532257c587dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Initialize the model\n",
    "model = MultiVideoHybridMVit2(num_classes=10, n=3)\n",
    "# Example input: [batch_size, num_views, channels, depth, height, width]\n",
    "videos = torch.randn(4, 3, 3, 11, 224, 224)\n",
    "output = model(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360cebe-e31a-42ff-97c0-1cf5eca7b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "37633/96/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d609f-cbba-41f7-9c51-7bdb2e93e22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
