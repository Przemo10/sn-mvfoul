{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259dd82b-138d-49c8-aa57-1e7239626907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m190.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m729.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m525.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: psutil in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.2.2)\n",
      "Requirement already satisfied: wheel in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m682.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m300.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m379.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m382.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m769.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m510.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m447.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m430.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m604.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m999.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m408.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.3-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m689.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m693.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m831.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, safetensors, requests, regex, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, filelock, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets, evaluate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed accelerate-0.31.0 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 filelock-3.15.3 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.23.4 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4b35abe-fee7-4f53-9716-7ec22d467135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import VivitImageProcessor, VivitModel, VivitConfig\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae0109a-786b-4212-a65a-bd0b18d0b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Decode the video with PyAV decoder.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "\n",
    "    '''\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    container.seek(0)\n",
    "\n",
    "    start_index = indices[0]\n",
    "\n",
    "    end_index = indices[-1]\n",
    "\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "\n",
    "        if i > end_index:\n",
    "\n",
    "            break\n",
    "\n",
    "        if i >= start_index and i in indices:\n",
    "\n",
    "            frames.append(frame)\n",
    "\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Sample a given number of frame indices from the video.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "\n",
    "    '''\n",
    "\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "\n",
    "    start_idx = end_idx - converted_len\n",
    "\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530787e2-c45b-46d7-a30d-28a73c0718ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f38c6c1e-6376-435d-a7a1-0ef6002db9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "model2 = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d187d8a-8e39-4b04-a1ed-868c7051307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VivitModel(\n",
       "  (embeddings): VivitEmbeddings(\n",
       "    (patch_embeddings): VivitTubeletEmbeddings(\n",
       "      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): VivitEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (2): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (3): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (4): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (5): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (6): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (7): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (8): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (9): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (10): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (11): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pooler): VivitPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf6ffac-53cd-4cf5-b031-610d94fe11f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VivitModel(\n",
       "  (embeddings): VivitEmbeddings(\n",
       "    (patch_embeddings): VivitTubeletEmbeddings(\n",
       "      (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): VivitEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (2): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (3): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (4): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (5): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (6): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (7): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (8): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (9): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (10): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (11): VivitLayer(\n",
       "        (attention): VivitAttention(\n",
       "          (attention): VivitSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): VivitSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VivitIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): FastGELUActivation()\n",
       "        )\n",
       "        (output): VivitOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pooler): VivitPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b89a470-efcb-4805-9919-edae94edea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.cls_token torch.Size([1, 1, 768])\n",
      "embeddings.position_embeddings torch.Size([1, 3137, 768])\n",
      "embeddings.patch_embeddings.projection.weight torch.Size([768, 3, 2, 16, 16])\n",
      "embeddings.patch_embeddings.projection.bias torch.Size([768])\n",
      "encoder.layer.0.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.0.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.0.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.0.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.1.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.1.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.1.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.1.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.2.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.2.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.2.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.2.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.3.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.3.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.3.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.3.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.4.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.4.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.4.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.4.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.5.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.5.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.5.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.5.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.6.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.6.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.6.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.6.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.7.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.7.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.7.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.7.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.8.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.8.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.8.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.8.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.9.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.9.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.9.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.9.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.10.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.10.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.10.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.10.layernorm_after.bias torch.Size([768])\n",
      "encoder.layer.11.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.attention.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.attention.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.attention.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.layernorm_before.weight torch.Size([768])\n",
      "encoder.layer.11.layernorm_before.bias torch.Size([768])\n",
      "encoder.layer.11.layernorm_after.weight torch.Size([768])\n",
      "encoder.layer.11.layernorm_after.bias torch.Size([768])\n",
      "layernorm.weight torch.Size([768])\n",
      "layernorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "  print(name ,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c057843e-8820-480b-a00d-d6116f7911cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_config = VivitConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59d8488a-c19a-4ba5-9b4a-0704face2df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VivitConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu_fast\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-06,\n",
       "  \"model_type\": \"vivit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_frames\": 32,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"tubelet_size\": [\n",
       "    2,\n",
       "    16,\n",
       "    16\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6303d3-bcf0-40eb-981f-e101da8e5b4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vars/lib/python3.9/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'attention'"
     ]
    }
   ],
   "source": [
    "model2.encoder.layer.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33f6b99-318d-419c-8f7c-4cbef250b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.7-py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m399.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from timm) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: pyyaml in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from timm) (0.23.4)\n",
      "Requirement already satisfied: safetensors in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (3.15.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (24.0)\n",
      "Requirement already satisfied: requests in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from huggingface_hub->timm) (4.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch->timm) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch->timm) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch->timm) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torch->timm) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->timm) (68.2.2)\n",
      "Requirement already satisfied: wheel in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->timm) (0.41.2)\n",
      "Requirement already satisfied: numpy in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torchvision->timm) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/przemo10/miniconda3/envs/vars/lib/python3.9/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Downloading timm-1.0.7-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m761.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947aedf3-c6b1-40d6-ac9e-c134f8f87ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9ae4e8f44842daaac1a3caa463770d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/146M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model('vit_small_r26_s32_224.augreg_in21k_ft_in1k', pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d7e587-8ebe-429f-878c-55f8b5f0ae7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): HybridEmbed(\n",
       "    (backbone): ResNetV2(\n",
       "      (stem): Sequential(\n",
       "        (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "        (norm): GroupNormAct(\n",
       "          32, 64, eps=1e-05, affine=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "      (stages): Sequential(\n",
       "        (0): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 256, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 512, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 1024, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 2048, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 2048, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 2048, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Identity()\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (proj): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6864a46-ddec-458a-8060-ce748f351ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 384])\n",
      "pos_embed torch.Size([1, 50, 384])\n",
      "patch_embed.backbone.stem.conv.weight torch.Size([64, 3, 7, 7])\n",
      "patch_embed.backbone.stem.norm.weight torch.Size([64])\n",
      "patch_embed.backbone.stem.norm.bias torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.0.downsample.conv.weight torch.Size([256, 64, 1, 1])\n",
      "patch_embed.backbone.stages.0.blocks.0.downsample.norm.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.0.blocks.0.downsample.norm.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.0.blocks.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm1.weight torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm1.bias torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm2.weight torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm2.bias torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm3.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.0.blocks.0.norm3.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.0.blocks.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm1.weight torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm1.bias torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm2.weight torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm2.bias torch.Size([64])\n",
      "patch_embed.backbone.stages.0.blocks.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm3.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.0.blocks.1.norm3.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.1.blocks.0.downsample.conv.weight torch.Size([512, 256, 1, 1])\n",
      "patch_embed.backbone.stages.1.blocks.0.downsample.norm.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.1.blocks.0.downsample.norm.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.1.blocks.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm1.weight torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm1.bias torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm2.weight torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm2.bias torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm3.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.1.blocks.0.norm3.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.1.blocks.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm1.weight torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm1.bias torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm2.weight torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm2.bias torch.Size([128])\n",
      "patch_embed.backbone.stages.1.blocks.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm3.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.1.blocks.1.norm3.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.2.blocks.0.downsample.conv.weight torch.Size([1024, 512, 1, 1])\n",
      "patch_embed.backbone.stages.2.blocks.0.downsample.norm.weight torch.Size([1024])\n",
      "patch_embed.backbone.stages.2.blocks.0.downsample.norm.bias torch.Size([1024])\n",
      "patch_embed.backbone.stages.2.blocks.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm1.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm1.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm2.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm2.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm3.weight torch.Size([1024])\n",
      "patch_embed.backbone.stages.2.blocks.0.norm3.bias torch.Size([1024])\n",
      "patch_embed.backbone.stages.2.blocks.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm1.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm1.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm2.weight torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm2.bias torch.Size([256])\n",
      "patch_embed.backbone.stages.2.blocks.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm3.weight torch.Size([1024])\n",
      "patch_embed.backbone.stages.2.blocks.1.norm3.bias torch.Size([1024])\n",
      "patch_embed.backbone.stages.3.blocks.0.downsample.conv.weight torch.Size([2048, 1024, 1, 1])\n",
      "patch_embed.backbone.stages.3.blocks.0.downsample.norm.weight torch.Size([2048])\n",
      "patch_embed.backbone.stages.3.blocks.0.downsample.norm.bias torch.Size([2048])\n",
      "patch_embed.backbone.stages.3.blocks.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm1.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm1.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm2.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm2.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm3.weight torch.Size([2048])\n",
      "patch_embed.backbone.stages.3.blocks.0.norm3.bias torch.Size([2048])\n",
      "patch_embed.backbone.stages.3.blocks.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm1.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm1.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm2.weight torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm2.bias torch.Size([512])\n",
      "patch_embed.backbone.stages.3.blocks.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm3.weight torch.Size([2048])\n",
      "patch_embed.backbone.stages.3.blocks.1.norm3.bias torch.Size([2048])\n",
      "patch_embed.proj.weight torch.Size([384, 2048, 1, 1])\n",
      "patch_embed.proj.bias torch.Size([384])\n",
      "blocks.0.norm1.weight torch.Size([384])\n",
      "blocks.0.norm1.bias torch.Size([384])\n",
      "blocks.0.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.0.attn.qkv.bias torch.Size([1152])\n",
      "blocks.0.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.0.attn.proj.bias torch.Size([384])\n",
      "blocks.0.norm2.weight torch.Size([384])\n",
      "blocks.0.norm2.bias torch.Size([384])\n",
      "blocks.0.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.0.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.0.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.0.mlp.fc2.bias torch.Size([384])\n",
      "blocks.1.norm1.weight torch.Size([384])\n",
      "blocks.1.norm1.bias torch.Size([384])\n",
      "blocks.1.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.1.attn.qkv.bias torch.Size([1152])\n",
      "blocks.1.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.1.attn.proj.bias torch.Size([384])\n",
      "blocks.1.norm2.weight torch.Size([384])\n",
      "blocks.1.norm2.bias torch.Size([384])\n",
      "blocks.1.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.1.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.1.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.1.mlp.fc2.bias torch.Size([384])\n",
      "blocks.2.norm1.weight torch.Size([384])\n",
      "blocks.2.norm1.bias torch.Size([384])\n",
      "blocks.2.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.2.attn.qkv.bias torch.Size([1152])\n",
      "blocks.2.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.2.attn.proj.bias torch.Size([384])\n",
      "blocks.2.norm2.weight torch.Size([384])\n",
      "blocks.2.norm2.bias torch.Size([384])\n",
      "blocks.2.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.2.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.2.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.2.mlp.fc2.bias torch.Size([384])\n",
      "blocks.3.norm1.weight torch.Size([384])\n",
      "blocks.3.norm1.bias torch.Size([384])\n",
      "blocks.3.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.3.attn.qkv.bias torch.Size([1152])\n",
      "blocks.3.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.3.attn.proj.bias torch.Size([384])\n",
      "blocks.3.norm2.weight torch.Size([384])\n",
      "blocks.3.norm2.bias torch.Size([384])\n",
      "blocks.3.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.3.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.3.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.3.mlp.fc2.bias torch.Size([384])\n",
      "blocks.4.norm1.weight torch.Size([384])\n",
      "blocks.4.norm1.bias torch.Size([384])\n",
      "blocks.4.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.4.attn.qkv.bias torch.Size([1152])\n",
      "blocks.4.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.4.attn.proj.bias torch.Size([384])\n",
      "blocks.4.norm2.weight torch.Size([384])\n",
      "blocks.4.norm2.bias torch.Size([384])\n",
      "blocks.4.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.4.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.4.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.4.mlp.fc2.bias torch.Size([384])\n",
      "blocks.5.norm1.weight torch.Size([384])\n",
      "blocks.5.norm1.bias torch.Size([384])\n",
      "blocks.5.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.5.attn.qkv.bias torch.Size([1152])\n",
      "blocks.5.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.5.attn.proj.bias torch.Size([384])\n",
      "blocks.5.norm2.weight torch.Size([384])\n",
      "blocks.5.norm2.bias torch.Size([384])\n",
      "blocks.5.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.5.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.5.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.5.mlp.fc2.bias torch.Size([384])\n",
      "blocks.6.norm1.weight torch.Size([384])\n",
      "blocks.6.norm1.bias torch.Size([384])\n",
      "blocks.6.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.6.attn.qkv.bias torch.Size([1152])\n",
      "blocks.6.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.6.attn.proj.bias torch.Size([384])\n",
      "blocks.6.norm2.weight torch.Size([384])\n",
      "blocks.6.norm2.bias torch.Size([384])\n",
      "blocks.6.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.6.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.6.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.6.mlp.fc2.bias torch.Size([384])\n",
      "blocks.7.norm1.weight torch.Size([384])\n",
      "blocks.7.norm1.bias torch.Size([384])\n",
      "blocks.7.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.7.attn.qkv.bias torch.Size([1152])\n",
      "blocks.7.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.7.attn.proj.bias torch.Size([384])\n",
      "blocks.7.norm2.weight torch.Size([384])\n",
      "blocks.7.norm2.bias torch.Size([384])\n",
      "blocks.7.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.7.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.7.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.7.mlp.fc2.bias torch.Size([384])\n",
      "blocks.8.norm1.weight torch.Size([384])\n",
      "blocks.8.norm1.bias torch.Size([384])\n",
      "blocks.8.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.8.attn.qkv.bias torch.Size([1152])\n",
      "blocks.8.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.8.attn.proj.bias torch.Size([384])\n",
      "blocks.8.norm2.weight torch.Size([384])\n",
      "blocks.8.norm2.bias torch.Size([384])\n",
      "blocks.8.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.8.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.8.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.8.mlp.fc2.bias torch.Size([384])\n",
      "blocks.9.norm1.weight torch.Size([384])\n",
      "blocks.9.norm1.bias torch.Size([384])\n",
      "blocks.9.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.9.attn.qkv.bias torch.Size([1152])\n",
      "blocks.9.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.9.attn.proj.bias torch.Size([384])\n",
      "blocks.9.norm2.weight torch.Size([384])\n",
      "blocks.9.norm2.bias torch.Size([384])\n",
      "blocks.9.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.9.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.9.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.9.mlp.fc2.bias torch.Size([384])\n",
      "blocks.10.norm1.weight torch.Size([384])\n",
      "blocks.10.norm1.bias torch.Size([384])\n",
      "blocks.10.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.10.attn.qkv.bias torch.Size([1152])\n",
      "blocks.10.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.10.attn.proj.bias torch.Size([384])\n",
      "blocks.10.norm2.weight torch.Size([384])\n",
      "blocks.10.norm2.bias torch.Size([384])\n",
      "blocks.10.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.10.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.10.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.10.mlp.fc2.bias torch.Size([384])\n",
      "blocks.11.norm1.weight torch.Size([384])\n",
      "blocks.11.norm1.bias torch.Size([384])\n",
      "blocks.11.attn.qkv.weight torch.Size([1152, 384])\n",
      "blocks.11.attn.qkv.bias torch.Size([1152])\n",
      "blocks.11.attn.proj.weight torch.Size([384, 384])\n",
      "blocks.11.attn.proj.bias torch.Size([384])\n",
      "blocks.11.norm2.weight torch.Size([384])\n",
      "blocks.11.norm2.bias torch.Size([384])\n",
      "blocks.11.mlp.fc1.weight torch.Size([1536, 384])\n",
      "blocks.11.mlp.fc1.bias torch.Size([1536])\n",
      "blocks.11.mlp.fc2.weight torch.Size([384, 1536])\n",
      "blocks.11.mlp.fc2.bias torch.Size([384])\n",
      "norm.weight torch.Size([384])\n",
      "norm.bias torch.Size([384])\n",
      "head.weight torch.Size([1000, 384])\n",
      "head.bias torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name ,param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e4c0c6-86a0-4223-978f-36c2fea36d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = model.blocks[2]\n",
    "b2.attn.fused_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3fc5c68-55f8-4910-9680-2d5031bd6009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "  (q_norm): Identity()\n",
       "  (k_norm): Identity()\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc3b881f-08f9-4438-8705-5ef71ce6e60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4ea175-86f1-446e-8916-ae13edf908b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=384, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d4829e-0965-4343-9bd9-990537913079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-1.2115e-02, -7.6643e-03,  1.1881e-02,  2.6442e-03, -1.0855e-02,\n",
       "           2.4157e-03, -4.0606e-03, -1.0074e-02, -6.4589e-03,  1.5418e-02,\n",
       "          -6.5152e-03,  8.1977e-02,  1.6923e-03,  1.7749e-03,  4.8799e-03,\n",
       "          -9.6725e-03, -7.9814e-03, -1.4743e-03,  3.0378e-03,  3.0562e-03,\n",
       "          -2.2242e-03, -7.5510e-03, -7.7013e-03, -8.9757e-03, -4.6692e-04,\n",
       "           1.1451e-03, -5.2634e-03, -7.0081e-03, -1.5327e-02, -1.6745e-02,\n",
       "           2.1182e-03,  1.4652e-04, -1.5238e-03, -1.2412e-03, -1.0010e-02,\n",
       "           1.8737e-02,  2.0203e-02, -6.4820e-03, -8.1280e-03, -1.9151e-02,\n",
       "          -1.0828e-02,  3.8819e-03,  1.8269e-02, -5.9395e-03, -2.3163e-02,\n",
       "           2.6079e-02, -1.5846e-02,  3.0691e-02, -2.5788e-03, -4.9555e-03,\n",
       "           6.4525e-03,  1.3288e-02, -3.9641e-04,  6.5831e-04,  5.2235e-03,\n",
       "          -4.2631e-03,  9.4750e-02, -8.5485e-03, -4.6355e-03,  5.3769e-03,\n",
       "          -1.2403e-02, -1.0681e-02,  1.6142e-02, -2.0484e-02, -6.5551e-03,\n",
       "           1.1311e-02,  1.3590e-02, -2.5746e-02, -6.5690e-03, -1.7357e-02,\n",
       "          -2.2136e-02,  2.2363e-03,  1.4724e-02,  1.4703e-02, -1.4251e-03,\n",
       "           5.3116e-03,  9.2291e-03, -7.9775e-03,  8.9441e-03, -6.0376e-03,\n",
       "          -1.3561e-02,  1.7177e-02, -1.4884e-02,  1.3638e-02,  1.0258e-02,\n",
       "          -2.5621e-02, -9.4301e-04, -1.6836e-02,  1.1524e-02,  6.8282e-03,\n",
       "           8.3580e-03, -1.3646e-02, -5.4212e-03,  3.6871e-03, -1.5572e-02,\n",
       "          -6.9756e-02,  1.4105e-02, -8.0193e-03,  2.7344e-03, -3.4142e-03,\n",
       "           1.6437e-03, -9.9834e-03,  9.4324e-03,  1.6750e-02,  4.3360e-02,\n",
       "           3.1806e-02,  1.4890e-02, -1.1945e-02, -2.4428e-03,  2.5505e-03,\n",
       "           1.5102e-02, -9.5524e-03,  2.4348e-03, -1.4806e-02, -4.6448e-03,\n",
       "          -1.5100e-02,  1.1454e-03,  1.5653e-02, -1.2022e-02, -9.2781e-03,\n",
       "           6.9601e-03,  6.2300e-03, -9.6599e-03,  8.3179e-03,  1.4775e-02,\n",
       "          -8.9464e-04,  8.6069e-03,  6.9428e-03,  1.2937e-02,  1.1503e-02,\n",
       "           3.2602e-03, -5.8951e-03,  2.0971e-03,  1.7922e-02,  1.9154e-03,\n",
       "           1.2739e-02, -2.4581e-02, -6.2800e-03, -1.3083e-01,  2.4932e-02,\n",
       "          -1.2793e-02, -1.9001e-02, -5.8851e-03,  2.0348e-03, -1.3741e-02,\n",
       "           2.5233e-02, -2.8749e-02,  1.8817e-02, -9.9225e-03,  2.4072e-02,\n",
       "          -2.3020e-02, -5.1975e-03, -3.8891e-02,  3.3996e-03,  1.6969e-02,\n",
       "           1.0263e-02, -1.3249e-02,  9.6561e-03,  4.6764e-03, -1.0092e-02,\n",
       "           2.6074e-03,  1.0443e-02, -9.9262e-03,  8.0930e-03,  6.0224e-03,\n",
       "           1.6478e-02, -7.1039e-02,  4.4836e-03, -7.1244e-02,  8.0289e-03,\n",
       "           1.3768e-03,  7.5216e-03, -7.2197e-03, -4.5304e-03, -8.0816e-03,\n",
       "          -2.0036e-02, -9.7764e-03,  2.1697e-02, -1.0930e-02, -3.6798e-03,\n",
       "          -9.7796e-03, -1.1571e-02,  1.1956e-02,  2.3949e-03, -2.0821e-02,\n",
       "           1.4632e-02, -1.2362e-02, -2.7138e-02,  4.4676e-03, -1.9577e-02,\n",
       "          -2.2281e-02, -2.0773e-02, -1.4039e-02,  4.4516e-04, -8.5596e-03,\n",
       "          -1.5460e-02,  8.5399e-03, -4.5195e-03, -2.6146e-03, -3.4571e-02,\n",
       "           7.3806e-03, -4.8051e-03, -1.8945e-02, -7.8349e-03,  4.3231e-02,\n",
       "           8.9145e-04, -8.7538e-03, -9.7804e-03, -1.2596e-02, -1.1662e-02,\n",
       "           1.3042e-02,  6.3527e-03, -4.4412e-03,  4.0733e-04, -9.8808e-02,\n",
       "           6.0572e-03, -1.4932e-02,  1.0662e-03, -2.1756e-02, -1.6808e-02,\n",
       "          -2.4373e-02, -1.1649e-02,  6.9727e-03,  1.1347e-01, -1.0390e-02,\n",
       "           2.5070e-02, -8.8472e-04, -3.8817e-03, -1.7150e-02,  5.7650e-03,\n",
       "           1.4979e-02, -2.1226e-03,  4.7186e-03,  6.8254e-03, -1.4330e-02,\n",
       "          -2.6164e-02, -1.8947e-02,  6.8201e-03,  1.5391e-02,  1.0300e-02,\n",
       "          -4.0252e-03, -5.0544e-03,  3.8382e-03, -1.4988e-02,  4.1879e-02,\n",
       "          -3.1643e-02,  1.1892e-02,  1.5860e-03, -1.5913e-02, -1.2015e-03,\n",
       "          -1.2848e-02, -8.2301e-03, -2.9824e-02,  4.2065e-03, -1.8833e-02,\n",
       "          -9.2145e-03,  6.8442e-03,  5.9963e-03, -1.4209e-02, -1.6945e-02,\n",
       "           3.0987e-02, -5.9904e-02, -7.2589e-03,  1.0956e-02, -9.5578e-03,\n",
       "          -4.4201e-03,  1.2047e-02,  6.0334e-02,  3.5679e-02,  1.9321e-02,\n",
       "          -2.6817e-02,  1.5340e-02,  9.4229e-03, -1.3713e-03,  1.0616e+01,\n",
       "           1.5871e-02,  6.0356e-03, -1.9857e-02,  1.1361e-02,  2.3332e-02,\n",
       "           2.4568e-02,  7.9647e-03, -2.5523e-03, -2.5079e-03,  8.7408e-03,\n",
       "           5.9442e-04,  6.5497e-03, -1.2017e-02, -3.0347e-02,  1.2374e-02,\n",
       "           1.3541e-02,  3.2225e-03, -6.2179e-03,  6.9579e-03, -9.9399e-03,\n",
       "          -2.0275e-02, -1.3335e-02,  4.5538e-03,  1.0155e-01, -1.7187e-02,\n",
       "           8.8586e-04,  2.0168e-02, -1.4867e-02, -8.9049e-03, -9.9512e-04,\n",
       "          -2.2573e-02,  5.9946e-02,  1.6915e-02, -2.7898e-02,  9.7034e-03,\n",
       "           2.3984e-03,  3.8994e-02,  6.3093e-03,  1.1529e-02, -1.7020e-02,\n",
       "           1.4591e-02,  3.4222e-02, -2.7339e-02,  5.6226e-02,  1.5456e-02,\n",
       "           1.3200e-02, -1.1365e-03,  8.9193e-04, -2.3039e-02, -1.2641e-02,\n",
       "           7.4924e-03,  2.3237e-03, -6.6915e-03, -1.2013e-02, -1.1915e-02,\n",
       "           3.8526e-02,  1.3308e-02,  1.2063e-02,  5.8280e-03, -3.3242e-02,\n",
       "           6.8138e-03, -1.0878e-02, -1.8358e-02,  8.6970e-03, -1.1394e-02,\n",
       "           5.5420e-01, -2.0937e-02,  4.1681e-04,  1.3538e-02,  8.7229e-03,\n",
       "           1.4084e-02, -6.5553e-02, -4.7870e-03, -2.2302e-03, -1.8851e-02,\n",
       "          -1.1445e-02,  1.1631e-03,  8.5937e-03, -6.4131e-03,  2.6643e-03,\n",
       "           1.3131e-02,  2.2746e-02, -1.6200e-02, -3.6034e-03, -5.8428e-03,\n",
       "           1.3909e-02, -2.4464e-02, -1.3030e-02, -1.8873e-02,  4.2962e-05,\n",
       "          -1.1232e-02,  2.1964e-03, -1.2082e-03, -2.7271e-02, -4.2070e-01,\n",
       "          -6.5919e-03, -2.7514e-03,  5.0394e-03,  6.0197e-03, -1.5812e-03,\n",
       "          -2.1474e-02,  2.1095e-02, -1.1368e-02, -7.9451e-03,  3.5951e-04,\n",
       "          -5.5632e-03,  7.9054e-03,  5.9895e-02, -1.5135e-03]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c292ace-1932-43eb-be76-070be818655c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39membeded_dim\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.embeded_dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
